// Copyright 2017 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

// Lowering arithmetic
(Add64  x y) -> (ADD  x y)
(AddPtr x y) -> (ADD  x y)
(Add32  x y) -> (ADD x y)
(Add16  x y) -> (ADD x y)
(Add8   x y) -> (ADD x y)
(Add32F x y) -> (FADDS x y)
(Add64F x y) -> (FADDD x y)

(SubPtr x y) -> (SUB x y)
(Sub64 x y) -> (SUB x y)
(Sub32 x y) -> (SUB x y)
(Sub16 x y) -> (SUB x y)
(Sub8 x y) -> (SUB x y)
(Sub32F x y) -> (FSUBS x y)
(Sub64F x y) -> (FSUBD x y)

(Mul64 x y) -> (MULD x y)
(Mul32 x y) -> (MULD x y)
(Mul16 x y) -> (MULD x y)
(Mul8 x y) -> (MULD x y)
(Mul32F x y) -> (FMULS x y)
(Mul64F x y) -> (FMULD x y)

(Div64 x y) -> (SDIVD x y)
(Div32 x y) -> (SDIVD x y)
(Div16 x y) -> (SDIVD x y)
(Div8 x y) -> (SDIVD x y)
(Div64u x y) -> (UDIVD x y)
(Div32u x y) -> (UDIVD x y)
(Div16u x y) -> (UDIVD x y)
(Div8u x y) -> (UDIVD x y)
(Div32F x y) -> (FDIVS x y)
(Div64F x y) -> (FDIVD x y)

(Mod8 x y) -> (Mod64 (SignExt8to64 x) (SignExt8to64 y))
(Mod8u x y) -> (Mod64u (ZeroExt8to64 x) (ZeroExt8to64 y))
(Mod16 x y) -> (Mod64 (SignExt16to64 x) (SignExt16to64 y))
(Mod16u x y) -> (Mod64u (ZeroExt16to64 x) (ZeroExt16to64 y))
(Mod32 x y) -> (Mod64 (SignExt32to64 x) (SignExt32to64 y))
(Mod32u x y) -> (Mod64u (ZeroExt32to64 x) (ZeroExt32to64 y))
(Mod64 x y) -> (SUB x (MULD y (SDIVD x y)))
(Mod64u x y) -> (SUB x (MULD y (UDIVD x y)))

(And64 x y) -> (AND x y)
(And32 x y) -> (AND x y)
(And16 x y) -> (AND x y)
(And8 x y) -> (AND x y)

(Or64 x y) -> (OR x y)
(Or32 x y) -> (OR x y)
(Or16 x y) -> (OR x y)
(Or8 x y) -> (OR x y)

(Xor64 x y) -> (XOR x y)
(Xor32 x y) -> (XOR x y)
(Xor16 x y) -> (XOR x y)
(Xor8 x y) -> (XOR x y)

// unary ops
(Neg64 x) -> (NEG x)
(Neg32 x) -> (NEG x)
(Neg16 x) -> (NEG x)
(Neg8 x) -> (NEG x)
(Neg32F x) -> (FNEGS x)
(Neg64F x) -> (FNEGD x)

(Sqrt x) -> (FSQRTD x)

// Lowering boolean ops
(AndB x y) -> (AND x y)
(OrB x y) -> (OR x y)
(Not x) -> (XORconst [1] x)

// Lowering extension
// Note: we always extend to 64 bits even though some ops don't need that many result bits.
(SignExt8to16  x) -> (MOVBreg x)
(SignExt8to32  x) -> (MOVBreg x)
(SignExt8to64  x) -> (MOVBreg x)
(SignExt16to32 x) -> (MOVHreg x)
(SignExt16to64 x) -> (MOVHreg x)
(SignExt32to64 x) -> (MOVWreg x)

(ZeroExt8to16  x) -> (MOVUBreg x)
(ZeroExt8to32  x) -> (MOVUBreg x)
(ZeroExt8to64  x) -> (MOVUBreg x)
(ZeroExt16to32 x) -> (MOVUHreg x)
(ZeroExt16to64 x) -> (MOVUHreg x)
(ZeroExt32to64 x) -> (MOVUWreg x)

(Trunc16to8  x) -> (MOVBreg x)
(Trunc32to8  x) -> (MOVBreg x)
(Trunc32to16 x) -> (MOVHreg x)
(Trunc64to8  x) -> (MOVBreg x)
(Trunc64to16 x) -> (MOVHreg x)
(Trunc64to32 x) -> (MOVWreg x)

// Lowering constants
(Const8   [val]) -> (MOVWconst [val])
(Const16  [val]) -> (MOVWconst [val])
(Const32  [val]) -> (MOVWconst [val])
(Const64  [val]) -> (MOVDconst [val])
(Const32F [val]) -> (FMOVSconst [val])
(Const64F [val]) -> (FMOVDconst [val])
(ConstNil) -> (MOVDconst [0])
(ConstBool [b]) -> (MOVWconst [b])

(OffPtr [off] ptr:(SP)) -> (MOVDaddr [off] ptr)
(OffPtr [off] ptr) -> (ADDconst [off] ptr)

(Addr {sym} base) -> (MOVDaddr {sym} base)

(Store [8] ptr val mem) -> (MOVDstore ptr val mem)

// calls
(StaticCall [argwid] {target} mem) -> (CALLstatic [argwid] {target} mem)
(DeferCall [argwid] mem) -> (CALLdefer [argwid] mem)
(GoCall [argwid] mem) -> (CALLgo [argwid] mem)

// loads
(Load <t> ptr mem) && t.IsBoolean() -> (MOVUBload ptr mem)
(Load <t> ptr mem) && (is8BitInt(t) && isSigned(t)) -> (MOVBload ptr mem)
(Load <t> ptr mem) && (is8BitInt(t) && !isSigned(t)) -> (MOVUBload ptr mem)
(Load <t> ptr mem) && (is16BitInt(t) && isSigned(t)) -> (MOVHload ptr mem)
(Load <t> ptr mem) && (is16BitInt(t) && !isSigned(t)) -> (MOVUHload ptr mem)
(Load <t> ptr mem) && (is32BitInt(t) && isSigned(t)) -> (MOVWload ptr mem)
(Load <t> ptr mem) && (is32BitInt(t) && !isSigned(t)) -> (MOVUWload ptr mem)
(Load <t> ptr mem) && (is64BitInt(t) || isPtr(t)) -> (MOVDload ptr mem)

// stores
(Store [1] ptr val mem) -> (MOVBstore ptr val mem)
(Store [2] ptr val mem) -> (MOVHstore ptr val mem)
(Store [4] ptr val mem) && !is32BitFloat(val.Type) -> (MOVWstore ptr val mem)
(Store [8] ptr val mem) && !is64BitFloat(val.Type) -> (MOVDstore ptr val mem)

// zeroing
(Zero [s] _ mem) && SizeAndAlign(s).Size() == 0 -> mem
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 1 -> (MOVBstore ptr (MOVDconst [0]) mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 2 -> (MOVHstore ptr (MOVDconst [0]) mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 4 -> (MOVWstore ptr (MOVDconst [0]) mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 8 -> (MOVDstore ptr (MOVDconst [0]) mem)

// comparisons
(Eq8 x y)  -> (Equal32 (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Eq16 x y) -> (Equal32 (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Eq32 x y) -> (Equal32 (CMP x y))
(Eq64 x y) -> (Equal64 (CMP x y))
(EqPtr x y) -> (Equal64 (CMP x y))
(Neq8 x y)  -> (NotEqual32 (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Neq16 x y) -> (NotEqual32 (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Neq32 x y) -> (NotEqual32 (CMP x y))
(Neq64 x y) -> (NotEqual64 (CMP x y))
(NeqPtr x y) -> (NotEqual64 (CMP x y))

(Less8 x y)  -> (LessThan32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Less16 x y) -> (LessThan32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Less32 x y) -> (LessThan32 (CMP x y))
(Less64 x y) -> (LessThan64 (CMP x y))
(Less8U x y)  -> (LessThan32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Less16U x y) -> (LessThan32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Less32U x y) -> (LessThan32U (CMP x y))
(Less64U x y) -> (LessThan64U (CMP x y))

(Leq8 x y)  -> (LessEqual32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Leq16 x y) -> (LessEqual32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Leq32 x y) -> (LessEqual32 (CMP x y))
(Leq64 x y) -> (LessEqual64 (CMP x y))
(Leq8U x y)  -> (LessEqual32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Leq16U x y) -> (LessEqual32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Leq32U x y) -> (LessEqual32U (CMP x y))
(Leq64U x y) -> (LessEqual64U (CMP x y))

(Greater8 x y)  -> (GreaterThan32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Greater16 x y) -> (GreaterThan32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Greater32 x y) -> (GreaterThan32 (CMP x y))
(Greater64 x y) -> (GreaterThan64 (CMP x y))
(Greater8U x y)  -> (GreaterThan32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Greater16U x y) -> (GreaterThan32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Greater32U x y) -> (GreaterThan32U (CMP x y))
(Greater64U x y) -> (GreaterThan64U (CMP x y))

(Geq8 x y)  -> (GreaterEqual32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Geq16 x y) -> (GreaterEqual32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Geq32 x y) -> (GreaterEqual32 (CMP x y))
(Geq64 x y) -> (GreaterEqual64 (CMP x y))
(Geq8U x y)  -> (GreaterEqual32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Geq16U x y) -> (GreaterEqual32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Geq32U x y) -> (GreaterEqual32U (CMP x y))
(Geq64U x y) -> (GreaterEqual64U (CMP x y))

// Absorb pseudo-ops into blocks.
(If (Equal32 cc) yes no) -> (EW cc yes no)
(If (NotEqual32 cc) yes no) -> (NEW cc yes no)
(If (LessThan32 cc) yes no) -> (LW cc yes no)
(If (LessThan32U cc) yes no) -> (CSW cc yes no)
(If (LessEqual32 cc) yes no) -> (LEW cc yes no)
(If (LessEqual32U cc) yes no) -> (LEUW cc yes no)
(If (GreaterThan32 cc) yes no) -> (GW cc yes no)
(If (GreaterThan32U cc) yes no) -> (GUW cc yes no)
(If (GreaterEqual32 cc) yes no) -> (GEW cc yes no)
(If (GreaterEqual32U cc) yes no) -> (CCW cc yes no)
(If (Equal64 cc) yes no) -> (ED cc yes no)
(If (NotEqual64 cc) yes no) -> (NED cc yes no)
(If (LessThan64 cc) yes no) -> (LD cc yes no)
(If (LessThan64U cc) yes no) -> (CSD cc yes no)
(If (LessEqual64 cc) yes no) -> (LED cc yes no)
(If (LessEqual64U cc) yes no) -> (LEUD cc yes no)
(If (GreaterThan64 cc) yes no) -> (GD cc yes no)
(If (GreaterThan64U cc) yes no) -> (GUD cc yes no)
(If (GreaterEqual64 cc) yes no) -> (GED cc yes no)
(If (GreaterEqual64U cc) yes no) -> (CCD cc yes no)

// Copyright 2017 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

// Lowering arithmetic
(Add64  x y) -> (ADD  x y)
(AddPtr x y) -> (ADD  x y)
(Add32  x y) -> (ADD x y)
(Add16  x y) -> (ADD x y)
(Add8   x y) -> (ADD x y)
(Add32F x y) -> (FADDS x y)
(Add64F x y) -> (FADDD x y)

(SubPtr x y) -> (SUB x y)
(Sub64 x y) -> (SUB x y)
(Sub32 x y) -> (SUB x y)
(Sub16 x y) -> (SUB x y)
(Sub8 x y) -> (SUB x y)
(Sub32F x y) -> (FSUBS x y)
(Sub64F x y) -> (FSUBD x y)

(Mul64 x y) -> (MULD x y)
(Mul32 x y) -> (MULD x y)
(Mul16 x y) -> (MULD x y)
(Mul8 x y) -> (MULD x y)
(Mul32F x y) -> (FMULS x y)
(Mul64F x y) -> (FMULD x y)

// TODO(shawn): handle high-multiply via mulx? vis3 has [u]mulxhi
(Hmul64 x y) -> (MULD x y)
(Hmul64u x y) -> (MULD x y)
(Hmul32 x y) -> (SRAconst (MULD <config.fe.TypeInt64()> x y) [32])
(Hmul32u x y) -> (SRAconst (MULD <config.fe.TypeUInt64()> x y) [32])
(Hmul16 x y) -> (SRAconst (MULD <config.fe.TypeInt32()> (SignExt16to32 x) (SignExt16to32 y)) [16])
(Hmul16u x y) -> (SRLconst (MULD <config.fe.TypeUInt32()> (ZeroExt16to32 x) (ZeroExt16to32 y)) [16])
(Hmul8 x y) -> (SRAconst (MULD <config.fe.TypeInt16()> (SignExt8to32 x) (SignExt8to32 y)) [8])
(Hmul8u x y) -> (SRLconst (MULD <config.fe.TypeUInt16()> (ZeroExt8to32 x) (ZeroExt8to32 y)) [8])

(Div64 x y) -> (SDIVD x y)
(Div32 x y) -> (SDIVD x y)
(Div16 x y) -> (SDIVD x y)
(Div8 x y) -> (SDIVD x y)
(Div64u x y) -> (UDIVD x y)
(Div32u x y) -> (UDIVD x y)
(Div16u x y) -> (UDIVD x y)
(Div8u x y) -> (UDIVD x y)
(Div32F x y) -> (FDIVS x y)
(Div64F x y) -> (FDIVD x y)

(Mod8 x y) -> (Mod64 (SignExt8to64 x) (SignExt8to64 y))
(Mod8u x y) -> (Mod64u (ZeroExt8to64 x) (ZeroExt8to64 y))
(Mod16 x y) -> (Mod64 (SignExt16to64 x) (SignExt16to64 y))
(Mod16u x y) -> (Mod64u (ZeroExt16to64 x) (ZeroExt16to64 y))
(Mod32 x y) -> (Mod64 (SignExt32to64 x) (SignExt32to64 y))
(Mod32u x y) -> (Mod64u (ZeroExt32to64 x) (ZeroExt32to64 y))
(Mod64 x y) -> (SUB x (MULD y (SDIVD x y)))
(Mod64u x y) -> (SUB x (MULD y (UDIVD x y)))

(Avg64u <t> x y) -> (ADD (ADD <t> (SRLconst <t> x [1]) (SRLconst <t> y [1])) (AND <t> (AND <t> x y) (MOVDconst [1])))

(And64 x y) -> (AND x y)
(And32 x y) -> (AND x y)
(And16 x y) -> (AND x y)
(And8 x y) -> (AND x y)

(Or64 x y) -> (OR x y)
(Or32 x y) -> (OR x y)
(Or16 x y) -> (OR x y)
(Or8 x y) -> (OR x y)

(Xor64 x y) -> (XOR x y)
(Xor32 x y) -> (XOR x y)
(Xor16 x y) -> (XOR x y)
(Xor8 x y) -> (XOR x y)

// unary ops
(Neg64 x) -> (NEG x)
(Neg32 x) -> (NEG x)
(Neg16 x) -> (NEG x)
(Neg8 x) -> (NEG x)
(Neg32F x) -> (FNEGS x)
(Neg64F x) -> (FNEGD x)

(Sqrt x) -> (FSQRTD x)

// Lowering boolean ops
(AndB x y) -> (AND x y)
(OrB x y) -> (OR x y)
(Not x) -> (XORconst [1] x)

// Lowering extension
// Note: we always extend to 64 bits even though some ops don't need that many result bits.
(SignExt8to16  x) -> (MOVBreg x)
(SignExt8to32  x) -> (MOVBreg x)
(SignExt8to64  x) -> (MOVBreg x)
(SignExt16to32 x) -> (MOVHreg x)
(SignExt16to64 x) -> (MOVHreg x)
(SignExt32to64 x) -> (MOVWreg x)

(ZeroExt8to16  x) -> (MOVUBreg x)
(ZeroExt8to32  x) -> (MOVUBreg x)
(ZeroExt8to64  x) -> (MOVUBreg x)
(ZeroExt16to32 x) -> (MOVUHreg x)
(ZeroExt16to64 x) -> (MOVUHreg x)
(ZeroExt32to64 x) -> (MOVUWreg x)

(Trunc16to8  x) -> (MOVBreg x)
(Trunc32to8  x) -> (MOVBreg x)
(Trunc32to16 x) -> (MOVHreg x)
(Trunc64to8  x) -> (MOVBreg x)
(Trunc64to16 x) -> (MOVHreg x)
(Trunc64to32 x) -> (MOVWreg x)

// Lowering constants
(Const8   [val]) -> (MOVWconst [val])
(Const16  [val]) -> (MOVWconst [val])
(Const32  [val]) -> (MOVWconst [val])
(Const64  [val]) -> (MOVDconst [val])
(Const32F [val]) -> (FMOVSconst [val])
(Const64F [val]) -> (FMOVDconst [val])
(ConstNil) -> (MOVDconst [0])
(ConstBool [b]) -> (MOVWconst [b])

(OffPtr [off] ptr:(SP)) -> (MOVDaddr [off] ptr)
(OffPtr [off] ptr) -> (ADDconst [off] ptr)

(Addr {sym} base) -> (MOVDaddr {sym} base)

(Store [8] ptr val mem) -> (MOVDstore ptr val mem)

// constant shifts
(Lsh64x64  x (MOVDconst [c])) && uint64(c) < 64 -> (SLLconst x [c])
(Rsh64x64  x (MOVDconst [c])) && uint64(c) < 64 -> (SRAconst x [c])
(Rsh64Ux64 x (MOVDconst [c])) && uint64(c) < 64 -> (SRLconst x [c])
(Lsh32x64  x (MOVDconst [c])) && uint64(c) < 32 -> (SLLconst x [c])
(Rsh32x64  x (MOVDconst [c])) && uint64(c) < 32 -> (SRAconst (SignExt32to64 x) [c])
(Rsh32Ux64 x (MOVDconst [c])) && uint64(c) < 32 -> (SRLconst (ZeroExt32to64 x) [c])
(Lsh16x64  x (MOVDconst [c])) && uint64(c) < 16 -> (SLLconst x [c])
(Rsh16x64  x (MOVDconst [c])) && uint64(c) < 16 -> (SRAconst (SignExt16to64 x) [c])
(Rsh16Ux64 x (MOVDconst [c])) && uint64(c) < 16 -> (SRLconst (ZeroExt16to64 x) [c])
(Lsh8x64   x (MOVDconst [c])) && uint64(c) < 8  -> (SLLconst x [c])
(Rsh8x64   x (MOVDconst [c])) && uint64(c) < 8  -> (SRAconst (SignExt8to64  x) [c])
(Rsh8Ux64  x (MOVDconst [c])) && uint64(c) < 8  -> (SRLconst (ZeroExt8to64  x) [c])

// large constant shifts
(Lsh64x64  _ (MOVDconst [c])) && uint64(c) >= 64 -> (MOVDconst [0])
(Rsh64Ux64 _ (MOVDconst [c])) && uint64(c) >= 64 -> (MOVDconst [0])
(Lsh32x64  _ (MOVDconst [c])) && uint64(c) >= 32 -> (MOVDconst [0])
(Rsh32Ux64 _ (MOVDconst [c])) && uint64(c) >= 32 -> (MOVDconst [0])
(Lsh16x64  _ (MOVDconst [c])) && uint64(c) >= 16 -> (MOVDconst [0])
(Rsh16Ux64 _ (MOVDconst [c])) && uint64(c) >= 16 -> (MOVDconst [0])
(Lsh8x64   _ (MOVDconst [c])) && uint64(c) >= 8  -> (MOVDconst [0])
(Rsh8Ux64  _ (MOVDconst [c])) && uint64(c) >= 8  -> (MOVDconst [0])

// large constant signed right shift, we leave the sign bit
(Rsh64x64 x (MOVDconst [c])) && uint64(c) >= 64 -> (SRAconst x [63])
(Rsh32x64 x (MOVDconst [c])) && uint64(c) >= 32 -> (SRAconst (SignExt32to64 x) [63])
(Rsh16x64 x (MOVDconst [c])) && uint64(c) >= 16 -> (SRAconst (SignExt16to64 x) [63])
(Rsh8x64  x (MOVDconst [c])) && uint64(c) >= 8  -> (SRAconst (SignExt8to64  x) [63])

// shifts
// hardware instruction uses only up to the low 6 bits of the shift
// so the S**max functions limit shifts to 63 and either zero the
// result or preserve the sign as required by Go semantics
(Lsh64x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh64x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh64x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh64x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Lsh32x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh32x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh32x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh32x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Lsh16x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh16x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh16x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh16x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Lsh8x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh8x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh8x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh8x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Rsh64Ux64 <t> x y) -> (SRLmax <t> x y [63])
(Rsh64Ux32 <t> x y) -> (SRLmax <t> x (ZeroExt32to64 y) [63])
(Rsh64Ux16 <t> x y) -> (SRLmax <t> x (ZeroExt16to64 y) [63])
(Rsh64Ux8  <t> x y) -> (SRLmax <t> x (ZeroExt8to64  y) [63])

(Rsh32Ux64 <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) y [63])
(Rsh32Ux32 <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) (ZeroExt32to64 y) [63])
(Rsh32Ux16 <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) (ZeroExt16to64 y) [63])
(Rsh32Ux8  <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) (ZeroExt8to64  y) [63])

(Rsh16Ux64 <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) y [63])
(Rsh16Ux32 <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) (ZeroExt32to64 y) [63])
(Rsh16Ux16 <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) (ZeroExt16to64 y) [63])
(Rsh16Ux8  <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) (ZeroExt8to64  y) [63])

(Rsh8Ux64 <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) y [63])
(Rsh8Ux32 <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) (ZeroExt32to64 y) [63])
(Rsh8Ux16 <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) (ZeroExt16to64 y) [63])
(Rsh8Ux8  <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) (ZeroExt8to64  y) [63])

(Rsh64x64 x y) -> (SRAmax x <y.Type> y [63])
(Rsh64x32 x y) -> (SRAmax x <y.Type> (ZeroExt32to64 y) [63])
(Rsh64x16 x y) -> (SRAmax x <y.Type> (ZeroExt16to64 y) [63])
(Rsh64x8  x y) -> (SRAmax x <y.Type> (ZeroExt8to64  y) [63])

(Rsh32x64 x y) -> (SRAmax (SignExt32to64 x) <y.Type> y [63])
(Rsh32x32 x y) -> (SRAmax (SignExt32to64 x) <y.Type> (ZeroExt32to64 y) [63])
(Rsh32x16 x y) -> (SRAmax (SignExt32to64 x) <y.Type> (ZeroExt16to64 y) [63])
(Rsh32x8  x y) -> (SRAmax (SignExt32to64 x) <y.Type> (ZeroExt8to64  y) [63])

(Rsh16x64 x y) -> (SRAmax (SignExt16to64 x) <y.Type> y [63])
(Rsh16x32 x y) -> (SRAmax (SignExt16to64 x) <y.Type> (ZeroExt32to64 y) [63])
(Rsh16x16 x y) -> (SRAmax (SignExt16to64 x) <y.Type> (ZeroExt16to64 y) [63])
(Rsh16x8  x y) -> (SRAmax (SignExt16to64 x) <y.Type> (ZeroExt8to64  y) [63])

(Rsh8x64 x y) -> (SRAmax (SignExt8to64 x) <y.Type> y [63])
(Rsh8x32 x y) -> (SRAmax (SignExt8to64 x) <y.Type> (ZeroExt32to64 y) [63])
(Rsh8x16 x y) -> (SRAmax (SignExt8to64 x) <y.Type> (ZeroExt16to64 y) [63])
(Rsh8x8  x y) -> (SRAmax (SignExt8to64 x) <y.Type> (ZeroExt8to64  y) [63])

// calls
(StaticCall [argwid] {target} mem) -> (CALLstatic [argwid] {target} mem)
(ClosureCall [argwid] entry closure mem) -> (CALLclosure [argwid] entry closure mem)
(DeferCall [argwid] mem) -> (CALLdefer [argwid] mem)
(GoCall [argwid] mem) -> (CALLgo [argwid] mem)

// loads
(Load <t> ptr mem) && t.IsBoolean() -> (MOVUBload ptr mem)
(Load <t> ptr mem) && (is8BitInt(t) && isSigned(t)) -> (MOVBload ptr mem)
(Load <t> ptr mem) && (is8BitInt(t) && !isSigned(t)) -> (MOVUBload ptr mem)
(Load <t> ptr mem) && (is16BitInt(t) && isSigned(t)) -> (MOVHload ptr mem)
(Load <t> ptr mem) && (is16BitInt(t) && !isSigned(t)) -> (MOVUHload ptr mem)
(Load <t> ptr mem) && (is32BitInt(t) && isSigned(t)) -> (MOVWload ptr mem)
(Load <t> ptr mem) && (is32BitInt(t) && !isSigned(t)) -> (MOVUWload ptr mem)
(Load <t> ptr mem) && (is64BitInt(t) || isPtr(t)) -> (MOVDload ptr mem)

// stores
(Store [1] ptr val mem) -> (MOVBstore ptr val mem)
(Store [2] ptr val mem) -> (MOVHstore ptr val mem)
(Store [4] ptr val mem) && !is32BitFloat(val.Type) -> (MOVWstore ptr val mem)
(Store [8] ptr val mem) && !is64BitFloat(val.Type) -> (MOVDstore ptr val mem)

// zeroing
(Zero [s] _ mem) && SizeAndAlign(s).Size() == 0 -> mem
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 1 -> (MOVBstore ptr (MOVDconst [0]) mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 2 -> (MOVHstore ptr (MOVDconst [0]) mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 4 -> (MOVWstore ptr (MOVDconst [0]) mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 8 -> (MOVDstore ptr (MOVDconst [0]) mem)

(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 3 ->
	(MOVBstore [2] ptr (MOVDconst [0])
		(MOVHstore ptr (MOVDconst [0]) mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 5 ->
	(MOVBstore [4] ptr (MOVDconst [0])
		(MOVWstore ptr (MOVDconst [0]) mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 6 ->
	(MOVHstore [4] ptr (MOVDconst [0])
		(MOVWstore ptr (MOVDconst [0]) mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 7 ->
	(MOVBstore [6] ptr (MOVDconst [0])
		(MOVHstore [4] ptr (MOVDconst [0])
			(MOVWstore ptr (MOVDconst [0]) mem)))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 12 ->
	(MOVWstore [8] ptr (MOVDconst [0])
		(MOVDstore ptr (MOVDconst [0]) mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 16 ->
	(MOVDstore [8] ptr (MOVDconst [0])
		(MOVDstore ptr (MOVDconst [0]) mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 24 ->
	(MOVDstore [16] ptr (MOVDconst [0])
		(MOVDstore [8] ptr (MOVDconst [0])
			(MOVDstore ptr (MOVDconst [0]) mem)))

// strip off fractional word zeroing
(Zero [s] ptr mem) && SizeAndAlign(s).Size()%8 != 0 && SizeAndAlign(s).Size() > 8 ->
	(Zero [MakeSizeAndAlign(SizeAndAlign(s).Size()%8, 1).Int64()]
		(OffPtr <ptr.Type> ptr [SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8])
		(Zero [MakeSizeAndAlign(SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8, 1).Int64()] ptr mem))

// medium and large zeroing uses a loop; no duff's device implemented yet
(Zero [s] ptr mem)
	&& SizeAndAlign(s).Size()%8 == 0 && (SizeAndAlign(s).Size() > 24 || config.noDuffDevice) ->
	(LoweredZero
		ptr
		(ADDconst <ptr.Type> [SizeAndAlign(s).Size()-moveSize(SizeAndAlign(s).Align(), config)] ptr)
		mem)

// moves
(Move [s] _ _ mem) && SizeAndAlign(s).Size() == 0 -> mem
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 1 -> (MOVBstore dst (MOVUBload src mem) mem)
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 2 -> (MOVHstore dst (MOVUHload src mem) mem)
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 4 -> (MOVWstore dst (MOVUWload src mem) mem)
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 8 -> (MOVDstore dst (MOVDload src mem) mem)

(Move [s] dst src mem) && SizeAndAlign(s).Size() == 3 ->
	(MOVBstore [2] dst (MOVUBload [2] src mem)
		(MOVHstore dst (MOVUHload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 5 ->
	(MOVBstore [4] dst (MOVUBload [4] src mem)
		(MOVWstore dst (MOVUWload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 6 ->
	(MOVHstore [4] dst (MOVUHload [4] src mem)
		(MOVWstore dst (MOVUWload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 7 ->
	(MOVBstore [6] dst (MOVUBload [6] src mem)
		(MOVHstore [4] dst (MOVUHload [4] src mem)
			(MOVWstore dst (MOVUWload src mem) mem)))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 12 ->
	(MOVWstore [8] dst (MOVUWload [8] src mem)
		(MOVDstore dst (MOVDload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 16 ->
	(MOVDstore [8] dst (MOVDload [8] src mem)
		(MOVDstore dst (MOVDload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 24 ->
	(MOVDstore [16] dst (MOVDload [16] src mem)
		(MOVDstore [8] dst (MOVDload [8] src mem)
			(MOVDstore dst (MOVDload src mem) mem)))

// strip off fractional word move
(Move [s] dst src mem) && SizeAndAlign(s).Size()%8 != 0 && SizeAndAlign(s).Size() > 8 ->
	(Move [MakeSizeAndAlign(SizeAndAlign(s).Size()%8, 1).Int64()]
		(OffPtr <dst.Type> dst [SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8])
		(OffPtr <src.Type> src [SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8])
		(Move [MakeSizeAndAlign(SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8, 1).Int64()] dst src mem))

// large move uses a loop
// DUFFCOPY is not implemented on SPARC64 (TODO)
(Move [s] dst src mem)
	&& SizeAndAlign(s).Size() > 24 && SizeAndAlign(s).Size()%8 == 0 ->
	(LoweredMove
		dst
		src
		(ADDconst <src.Type> src [SizeAndAlign(s).Size()-moveSize(SizeAndAlign(s).Align(), config)])
		mem)

// comparisons
(Eq8 x y)  -> (Equal32 (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Eq16 x y) -> (Equal32 (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Eq32 x y) -> (Equal32 (CMP x y))
(Eq64 x y) -> (Equal64 (CMP x y))
(EqPtr x y) -> (Equal64 (CMP x y))
(Neq8 x y)  -> (NotEqual32 (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Neq16 x y) -> (NotEqual32 (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Neq32 x y) -> (NotEqual32 (CMP x y))
(Neq64 x y) -> (NotEqual64 (CMP x y))
(NeqPtr x y) -> (NotEqual64 (CMP x y))

(Less8 x y)  -> (LessThan32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Less16 x y) -> (LessThan32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Less32 x y) -> (LessThan32 (CMP x y))
(Less64 x y) -> (LessThan64 (CMP x y))
(Less8U x y)  -> (LessThan32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Less16U x y) -> (LessThan32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Less32U x y) -> (LessThan32U (CMP x y))
(Less64U x y) -> (LessThan64U (CMP x y))

(Leq8 x y)  -> (LessEqual32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Leq16 x y) -> (LessEqual32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Leq32 x y) -> (LessEqual32 (CMP x y))
(Leq64 x y) -> (LessEqual64 (CMP x y))
(Leq8U x y)  -> (LessEqual32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Leq16U x y) -> (LessEqual32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Leq32U x y) -> (LessEqual32U (CMP x y))
(Leq64U x y) -> (LessEqual64U (CMP x y))

(Greater8 x y)  -> (GreaterThan32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Greater16 x y) -> (GreaterThan32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Greater32 x y) -> (GreaterThan32 (CMP x y))
(Greater64 x y) -> (GreaterThan64 (CMP x y))
(Greater8U x y)  -> (GreaterThan32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Greater16U x y) -> (GreaterThan32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Greater32U x y) -> (GreaterThan32U (CMP x y))
(Greater64U x y) -> (GreaterThan64U (CMP x y))

(Geq8 x y)  -> (GreaterEqual32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Geq16 x y) -> (GreaterEqual32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Geq32 x y) -> (GreaterEqual32 (CMP x y))
(Geq64 x y) -> (GreaterEqual64 (CMP x y))
(Geq8U x y)  -> (GreaterEqual32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Geq16U x y) -> (GreaterEqual32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Geq32U x y) -> (GreaterEqual32U (CMP x y))
(Geq64U x y) -> (GreaterEqual64U (CMP x y))

// checks
(NilCheck ptr mem) -> (LoweredNilCheck ptr mem)
(IsNonNil ptr) -> (NotEqual64 (CMPconst [0] ptr))
(IsInBounds idx len) -> (LessThan64U (CMP idx len))
(IsSliceInBounds idx len) -> (LessEqual64U (CMP idx len))

// Absorb pseudo-ops into blocks.
(If (Equal32 cc) yes no) -> (EW cc yes no)
(If (NotEqual32 cc) yes no) -> (NEW cc yes no)
(If (LessThan32 cc) yes no) -> (LW cc yes no)
(If (LessThan32U cc) yes no) -> (CSW cc yes no)
(If (LessEqual32 cc) yes no) -> (LEW cc yes no)
(If (LessEqual32U cc) yes no) -> (LEUW cc yes no)
(If (GreaterThan32 cc) yes no) -> (GW cc yes no)
(If (GreaterThan32U cc) yes no) -> (GUW cc yes no)
(If (GreaterEqual32 cc) yes no) -> (GEW cc yes no)
(If (GreaterEqual32U cc) yes no) -> (CCW cc yes no)
(If (Equal64 cc) yes no) -> (ED cc yes no)
(If (NotEqual64 cc) yes no) -> (NED cc yes no)
(If (LessThan64 cc) yes no) -> (LD cc yes no)
(If (LessThan64U cc) yes no) -> (CSD cc yes no)
(If (LessEqual64 cc) yes no) -> (LED cc yes no)
(If (LessEqual64U cc) yes no) -> (LEUD cc yes no)
(If (GreaterThan64 cc) yes no) -> (GD cc yes no)
(If (GreaterThan64U cc) yes no) -> (GUD cc yes no)
(If (GreaterEqual64 cc) yes no) -> (GED cc yes no)
(If (GreaterEqual64U cc) yes no) -> (CCD cc yes no)

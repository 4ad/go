// Copyright 2017 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

// Lowering arithmetic
(Add64  x y) -> (ADD  x y)
(AddPtr x y) -> (ADD  x y)
(Add32  x y) -> (ADD x y)
(Add16  x y) -> (ADD x y)
(Add8   x y) -> (ADD x y)
(Add32F x y) -> (FADDS x y)
(Add64F x y) -> (FADDD x y)

(SubPtr x y) -> (SUB x y)
(Sub64 x y) -> (SUB x y)
(Sub32 x y) -> (SUB x y)
(Sub16 x y) -> (SUB x y)
(Sub8 x y) -> (SUB x y)
(Sub32F x y) -> (FSUBS x y)
(Sub64F x y) -> (FSUBD x y)

(Mul64 x y) -> (MULD x y)
(Mul32 x y) -> (MULD x y)
(Mul16 x y) -> (MULD x y)
(Mul8 x y) -> (MULD x y)
(Mul32F x y) -> (FMULS x y)
(Mul64F x y) -> (FMULD x y)

// TODO(shawn): handle high-multiply via mulx? vis3 has [u]mulxhi
(Hmul64 x y) -> (MULD x y)
(Hmul64u x y) -> (MULD x y)
(Hmul32 x y) -> (SRAconst (MULD <config.fe.TypeInt64()> x y) [32])
(Hmul32u x y) -> (SRAconst (MULD <config.fe.TypeUInt64()> x y) [32])
(Hmul16 x y) -> (SRAconst (MULD <config.fe.TypeInt32()> (SignExt16to32 x) (SignExt16to32 y)) [16])
(Hmul16u x y) -> (SRLconst (MULD <config.fe.TypeUInt32()> (ZeroExt16to32 x) (ZeroExt16to32 y)) [16])
(Hmul8 x y) -> (SRAconst (MULD <config.fe.TypeInt16()> (SignExt8to32 x) (SignExt8to32 y)) [8])
(Hmul8u x y) -> (SRLconst (MULD <config.fe.TypeUInt16()> (ZeroExt8to32 x) (ZeroExt8to32 y)) [8])

(Div64 x y) -> (SDIVD x y)
(Div32 x y) -> (SDIVD x y)
(Div16 x y) -> (SDIVD x y)
(Div8 x y) -> (SDIVD x y)
(Div64u x y) -> (UDIVD x y)
(Div32u x y) -> (UDIVD x y)
(Div16u x y) -> (UDIVD x y)
(Div8u x y) -> (UDIVD x y)
(Div32F x y) -> (FDIVS x y)
(Div64F x y) -> (FDIVD x y)

(Mod8 x y) -> (Mod64 (SignExt8to64 x) (SignExt8to64 y))
(Mod8u x y) -> (Mod64u (ZeroExt8to64 x) (ZeroExt8to64 y))
(Mod16 x y) -> (Mod64 (SignExt16to64 x) (SignExt16to64 y))
(Mod16u x y) -> (Mod64u (ZeroExt16to64 x) (ZeroExt16to64 y))
(Mod32 x y) -> (Mod64 (SignExt32to64 x) (SignExt32to64 y))
(Mod32u x y) -> (Mod64u (ZeroExt32to64 x) (ZeroExt32to64 y))
(Mod64 x y) -> (SUB x (MULD y (SDIVD x y)))
(Mod64u x y) -> (SUB x (MULD y (UDIVD x y)))

(Avg64u <t> x y) -> (ADD (ADD <t> (SRLconst <t> x [1]) (SRLconst <t> y [1])) (AND <t> (AND <t> x y) (MOVDconst [1])))

(And64 x y) -> (AND x y)
(And32 x y) -> (AND x y)
(And16 x y) -> (AND x y)
(And8 x y) -> (AND x y)

(Or64 x y) -> (OR x y)
(Or32 x y) -> (OR x y)
(Or16 x y) -> (OR x y)
(Or8 x y) -> (OR x y)

(Xor64 x y) -> (XOR x y)
(Xor32 x y) -> (XOR x y)
(Xor16 x y) -> (XOR x y)
(Xor8 x y) -> (XOR x y)

// unary ops
(Neg64 x) -> (NEG x)
(Neg32 x) -> (NEG x)
(Neg16 x) -> (NEG x)
(Neg8 x) -> (NEG x)
(Neg32F x) -> (FNEGS x)
(Neg64F x) -> (FNEGD x)

(Sqrt x) -> (FSQRTD x)

(Com64 x) -> (XORconst [-1] x)
(Com32 x) -> (XORconst [-1] x)
(Com16 x) -> (XORconst [-1] x)
(Com8  x) -> (XORconst [-1] x)

// Lowering boolean ops
(AndB x y) -> (AND x y)
(OrB x y) -> (OR x y)
(EqB x y) -> (XOR (MOVDconst [1]) (XOR <config.fe.TypeBool()> x y))
(NeqB x y) -> (XOR x y)
(Not x) -> (XORconst [1] x)

// Lowering extension
// Note: we always extend to 64 bits even though some ops don't need that many result bits.
(SignExt8to16  x) -> (MOVBreg x)
(SignExt8to32  x) -> (MOVBreg x)
(SignExt8to64  x) -> (MOVBreg x)
(SignExt16to32 x) -> (MOVHreg x)
(SignExt16to64 x) -> (MOVHreg x)
(SignExt32to64 x) -> (MOVWreg x)

(ZeroExt8to16  x) -> (MOVUBreg x)
(ZeroExt8to32  x) -> (MOVUBreg x)
(ZeroExt8to64  x) -> (MOVUBreg x)
(ZeroExt16to32 x) -> (MOVUHreg x)
(ZeroExt16to64 x) -> (MOVUHreg x)
(ZeroExt32to64 x) -> (MOVUWreg x)

// float <-> int conversion
(Cvt32to32F x) -> (FITOS x)
(Cvt32to64F x) -> (FITOD x)
(Cvt64to32F x) -> (FXTOS x)
(Cvt64to64F x) -> (FXTOD x)
(Cvt32Uto32F x) -> (FITOS x)
(Cvt32Uto64F x) -> (FITOD x)
(Cvt64Uto32F x) -> (FXTOS x)
(Cvt64Uto64F x) -> (FXTOD x)
(Cvt32Fto32 x) -> (FSTOI x)
(Cvt64Fto32 x) -> (FDTOI x)
(Cvt32Fto64 x) -> (FSTOX x)
(Cvt64Fto64 x) -> (FDTOX x)
(Cvt32Fto32U x) -> (FSTOI x)
(Cvt64Fto32U x) -> (FDTOI x)
(Cvt32Fto64U x) -> (FSTOX x)
(Cvt64Fto64U x) -> (FDTOX x)
(Cvt32Fto64F x) -> (FSTOD x)
(Cvt64Fto32F x) -> (FDTOS x)

// truncations
(Trunc16to8  x) -> (MOVBreg x)
(Trunc32to8  x) -> (MOVBreg x)
(Trunc32to16 x) -> (MOVHreg x)
(Trunc64to8  x) -> (MOVBreg x)
(Trunc64to16 x) -> (MOVHreg x)
(Trunc64to32 x) -> (MOVWreg x)

// Lowering constants
(Const8   [val]) -> (MOVWconst [val])
(Const16  [val]) -> (MOVWconst [val])
(Const32  [val]) -> (MOVWconst [val])
(Const64  [val]) -> (MOVDconst [val])
(Const32F [val]) -> (FMOVSconst [val])
(Const64F [val]) -> (FMOVDconst [val])
(ConstNil) -> (MOVDconst [0])
(ConstBool [b]) -> (MOVWconst [b])

(OffPtr [off] ptr:(SP)) -> (MOVDaddr [off] ptr)
(OffPtr [off] ptr) -> (ADDconst [off] ptr)

(Addr {sym} base) -> (MOVDaddr {sym} base)

// XXX should be able to use MOVDconst below instead of Const64; why not?
// constant shifts
(Lsh64x64  x (MOVDconst [c])) && uint64(c) < 64 -> (SLLconst x [c])
(Rsh64x64  x (MOVDconst [c])) && uint64(c) < 64 -> (SRAconst x [c])
(Rsh64Ux64 x (MOVDconst [c])) && uint64(c) < 64 -> (SRLconst x [c])
(Lsh32x64  x (MOVDconst [c])) && uint64(c) < 32 -> (SLLconst x [c])
(Rsh32x64  x (MOVDconst [c])) && uint64(c) < 32 -> (SRAconst (SignExt32to64 x) [c])
(Rsh32Ux64 x (MOVDconst [c])) && uint64(c) < 32 -> (SRLconst (ZeroExt32to64 x) [c])
(Lsh16x64  x (MOVDconst [c])) && uint64(c) < 16 -> (SLLconst x [c])
(Rsh16x64  x (MOVDconst [c])) && uint64(c) < 16 -> (SRAconst (SignExt16to64 x) [c])
(Rsh16Ux64 x (MOVDconst [c])) && uint64(c) < 16 -> (SRLconst (ZeroExt16to64 x) [c])
(Lsh8x64   x (MOVDconst [c])) && uint64(c) < 8  -> (SLLconst x [c])
(Rsh8x64   x (MOVDconst [c])) && uint64(c) < 8  -> (SRAconst (SignExt8to64  x) [c])
(Rsh8Ux64  x (MOVDconst [c])) && uint64(c) < 8  -> (SRLconst (ZeroExt8to64  x) [c])

(Lsh64x64  x (Const64 [c])) && uint64(c) < 64 -> (SLLconst x [c])
(Rsh64x64  x (Const64 [c])) && uint64(c) < 64 -> (SRAconst x [c])
(Rsh64Ux64 x (Const64 [c])) && uint64(c) < 64 -> (SRLconst x [c])
(Lsh32x64  x (Const64 [c])) && uint64(c) < 32 -> (SLLconst x [c])
(Rsh32x64  x (Const64 [c])) && uint64(c) < 32 -> (SRAconst (SignExt32to64 x) [c])
(Rsh32Ux64 x (Const64 [c])) && uint64(c) < 32 -> (SRLconst (ZeroExt32to64 x) [c])
(Lsh16x64  x (Const64 [c])) && uint64(c) < 16 -> (SLLconst x [c])
(Rsh16x64  x (Const64 [c])) && uint64(c) < 16 -> (SRAconst (SignExt16to64 x) [c])
(Rsh16Ux64 x (Const64 [c])) && uint64(c) < 16 -> (SRLconst (ZeroExt16to64 x) [c])
(Lsh8x64   x (Const64 [c])) && uint64(c) < 8  -> (SLLconst x [c])
(Rsh8x64   x (Const64 [c])) && uint64(c) < 8  -> (SRAconst (SignExt8to64  x) [c])
(Rsh8Ux64  x (Const64 [c])) && uint64(c) < 8  -> (SRLconst (ZeroExt8to64  x) [c])

// large constant shifts
(Lsh64x64  _ (MOVDconst [c])) && uint64(c) >= 64 -> (MOVDconst [0])
(Rsh64Ux64 _ (MOVDconst [c])) && uint64(c) >= 64 -> (MOVDconst [0])
(Lsh32x64  _ (MOVDconst [c])) && uint64(c) >= 32 -> (MOVDconst [0])
(Rsh32Ux64 _ (MOVDconst [c])) && uint64(c) >= 32 -> (MOVDconst [0])
(Lsh16x64  _ (MOVDconst [c])) && uint64(c) >= 16 -> (MOVDconst [0])
(Rsh16Ux64 _ (MOVDconst [c])) && uint64(c) >= 16 -> (MOVDconst [0])
(Lsh8x64   _ (MOVDconst [c])) && uint64(c) >= 8  -> (MOVDconst [0])
(Rsh8Ux64  _ (MOVDconst [c])) && uint64(c) >= 8  -> (MOVDconst [0])

(Lsh64x64  _ (Const64 [c])) && uint64(c) >= 64 -> (Const64 [0])
(Rsh64Ux64 _ (Const64 [c])) && uint64(c) >= 64 -> (Const64 [0])
(Lsh32x64  _ (Const64 [c])) && uint64(c) >= 32 -> (Const64 [0])
(Rsh32Ux64 _ (Const64 [c])) && uint64(c) >= 32 -> (Const64 [0])
(Lsh16x64  _ (Const64 [c])) && uint64(c) >= 16 -> (Const64 [0])
(Rsh16Ux64 _ (Const64 [c])) && uint64(c) >= 16 -> (Const64 [0])
(Lsh8x64   _ (Const64 [c])) && uint64(c) >= 8  -> (Const64 [0])
(Rsh8Ux64  _ (Const64 [c])) && uint64(c) >= 8  -> (Const64 [0])

// large constant signed right shift, we leave the sign bit
(Rsh64x64 x (MOVDconst [c])) && uint64(c) >= 64 -> (SRAconst x [63])
(Rsh32x64 x (MOVDconst [c])) && uint64(c) >= 32 -> (SRAconst (SignExt32to64 x) [63])
(Rsh16x64 x (MOVDconst [c])) && uint64(c) >= 16 -> (SRAconst (SignExt16to64 x) [63])
(Rsh8x64  x (MOVDconst [c])) && uint64(c) >= 8  -> (SRAconst (SignExt8to64  x) [63])

(Rsh64x64 x (Const64 [c])) && uint64(c) >= 64 -> (SRAconst x [63])
(Rsh32x64 x (Const64 [c])) && uint64(c) >= 32 -> (SRAconst (SignExt32to64 x) [63])
(Rsh16x64 x (Const64 [c])) && uint64(c) >= 16 -> (SRAconst (SignExt16to64 x) [63])
(Rsh8x64  x (Const64 [c])) && uint64(c) >= 8  -> (SRAconst (SignExt8to64  x) [63])

// shifts
// hardware instruction uses only up to the low 6 bits of the shift
// so the S**max functions limit shifts to 63 and either zero the
// result or preserve the sign as required by Go semantics
(Lsh64x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh64x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh64x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh64x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Lsh32x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh32x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh32x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh32x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Lsh16x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh16x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh16x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh16x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Lsh8x64 <t> x y) -> (SLLmax <t> x y [63])
(Lsh8x32 <t> x y) -> (SLLmax <t> x (ZeroExt32to64 y) [63])
(Lsh8x16 <t> x y) -> (SLLmax <t> x (ZeroExt16to64 y) [63])
(Lsh8x8  <t> x y) -> (SLLmax <t> x (ZeroExt8to64  y) [63])

(Rsh64Ux64 <t> x y) -> (SRLmax <t> x y [63])
(Rsh64Ux32 <t> x y) -> (SRLmax <t> x (ZeroExt32to64 y) [63])
(Rsh64Ux16 <t> x y) -> (SRLmax <t> x (ZeroExt16to64 y) [63])
(Rsh64Ux8  <t> x y) -> (SRLmax <t> x (ZeroExt8to64  y) [63])

(Rsh32Ux64 <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) y [63])
(Rsh32Ux32 <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) (ZeroExt32to64 y) [63])
(Rsh32Ux16 <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) (ZeroExt16to64 y) [63])
(Rsh32Ux8  <t> x y) -> (SRLmax <t> (ZeroExt32to64 x) (ZeroExt8to64  y) [63])

(Rsh16Ux64 <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) y [63])
(Rsh16Ux32 <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) (ZeroExt32to64 y) [63])
(Rsh16Ux16 <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) (ZeroExt16to64 y) [63])
(Rsh16Ux8  <t> x y) -> (SRLmax <t> (ZeroExt16to64 x) (ZeroExt8to64  y) [63])

(Rsh8Ux64 <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) y [63])
(Rsh8Ux32 <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) (ZeroExt32to64 y) [63])
(Rsh8Ux16 <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) (ZeroExt16to64 y) [63])
(Rsh8Ux8  <t> x y) -> (SRLmax <t> (ZeroExt8to64 x) (ZeroExt8to64  y) [63])

(Rsh64x64 x y) -> (SRAmax x <y.Type> y [63])
(Rsh64x32 x y) -> (SRAmax x <y.Type> (ZeroExt32to64 y) [63])
(Rsh64x16 x y) -> (SRAmax x <y.Type> (ZeroExt16to64 y) [63])
(Rsh64x8  x y) -> (SRAmax x <y.Type> (ZeroExt8to64  y) [63])

(Rsh32x64 x y) -> (SRAmax (SignExt32to64 x) <y.Type> y [63])
(Rsh32x32 x y) -> (SRAmax (SignExt32to64 x) <y.Type> (ZeroExt32to64 y) [63])
(Rsh32x16 x y) -> (SRAmax (SignExt32to64 x) <y.Type> (ZeroExt16to64 y) [63])
(Rsh32x8  x y) -> (SRAmax (SignExt32to64 x) <y.Type> (ZeroExt8to64  y) [63])

(Rsh16x64 x y) -> (SRAmax (SignExt16to64 x) <y.Type> y [63])
(Rsh16x32 x y) -> (SRAmax (SignExt16to64 x) <y.Type> (ZeroExt32to64 y) [63])
(Rsh16x16 x y) -> (SRAmax (SignExt16to64 x) <y.Type> (ZeroExt16to64 y) [63])
(Rsh16x8  x y) -> (SRAmax (SignExt16to64 x) <y.Type> (ZeroExt8to64  y) [63])

(Rsh8x64 x y) -> (SRAmax (SignExt8to64 x) <y.Type> y [63])
(Rsh8x32 x y) -> (SRAmax (SignExt8to64 x) <y.Type> (ZeroExt32to64 y) [63])
(Rsh8x16 x y) -> (SRAmax (SignExt8to64 x) <y.Type> (ZeroExt16to64 y) [63])
(Rsh8x8  x y) -> (SRAmax (SignExt8to64 x) <y.Type> (ZeroExt8to64  y) [63])

// calls
(StaticCall [argwid] {target} mem) -> (CALLstatic [argwid] {target} mem)
(ClosureCall [argwid] entry closure mem) -> (CALLclosure [argwid] entry closure mem)
(DeferCall [argwid] mem) -> (CALLdefer [argwid] mem)
(GoCall [argwid] mem) -> (CALLgo [argwid] mem)
(InterCall [argwid] entry mem) -> (CALLinter [argwid] entry mem)

// loads
(Load <t> ptr mem) && t.IsBoolean() -> (MOVUBload ptr mem)
(Load <t> ptr mem) && (is8BitInt(t) && isSigned(t)) -> (MOVBload ptr mem)
(Load <t> ptr mem) && (is8BitInt(t) && !isSigned(t)) -> (MOVUBload ptr mem)
(Load <t> ptr mem) && (is16BitInt(t) && isSigned(t)) -> (MOVHload ptr mem)
(Load <t> ptr mem) && (is16BitInt(t) && !isSigned(t)) -> (MOVUHload ptr mem)
(Load <t> ptr mem) && (is32BitInt(t) && isSigned(t)) -> (MOVWload ptr mem)
(Load <t> ptr mem) && (is32BitInt(t) && !isSigned(t)) -> (MOVUWload ptr mem)
(Load <t> ptr mem) && (is64BitInt(t) || isPtr(t)) -> (MOVDload ptr mem)
(Load <t> ptr mem) && is32BitFloat(t) -> (FMOVSload ptr mem)
(Load <t> ptr mem) && is64BitFloat(t) -> (FMOVDload ptr mem)

// stores
(Store [1] ptr val mem) -> (MOVBstore ptr val mem)
(Store [2] ptr val mem) -> (MOVHstore ptr val mem)
(Store [4] ptr val mem) && !is32BitFloat(val.Type) -> (MOVWstore ptr val mem)
(Store [8] ptr val mem) && !is64BitFloat(val.Type) -> (MOVDstore ptr val mem)
(Store [4] ptr val mem) && is32BitFloat(val.Type) -> (FMOVSstore ptr val mem)
(Store [8] ptr val mem) && is64BitFloat(val.Type) -> (FMOVDstore ptr val mem)

// zeroing
(Zero [s] _ mem) && SizeAndAlign(s).Size() == 0 -> mem
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 1 -> (MOVBstorezero ptr mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 2 -> (MOVHstorezero ptr mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 4 -> (MOVWstorezero ptr mem)
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 8 -> (MOVDstorezero ptr mem)

(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 3 ->
	(MOVBstorezero [2] ptr
		(MOVHstorezero [0] ptr mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 5 ->
	(MOVBstorezero [4] ptr
		(MOVWstorezero [0] ptr mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 6 ->
	(MOVHstorezero [4] ptr
		(MOVWstorezero [0] ptr mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 7 ->
	(MOVBstorezero [6] ptr
		(MOVHstorezero [4] ptr
			(MOVWstorezero [0] ptr mem)))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 12 ->
	(MOVWstorezero [8] ptr
		(MOVDstorezero [0] ptr mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 16 ->
	(MOVDstorezero [8] ptr
		(MOVDstorezero [0] ptr mem))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 24 ->
	(MOVDstorezero [16] ptr
		(MOVDstorezero [8] ptr
			(MOVDstorezero [0] ptr mem)))
(Zero [s] ptr mem) && SizeAndAlign(s).Size() == 32 ->
	(MOVDstorezero [24] ptr
		(MOVDstorezero [16] ptr
			(MOVDstorezero [8] ptr
				(MOVDstorezero [0] ptr mem))))

// strip off fractional word zeroing
(Zero [s] ptr mem) && SizeAndAlign(s).Size()%8 != 0 && SizeAndAlign(s).Size() > 8 ->
	(Zero [MakeSizeAndAlign(SizeAndAlign(s).Size()%8, 1).Int64()]
		(OffPtr <ptr.Type> ptr [SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8])
		(Zero [MakeSizeAndAlign(SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8, 1).Int64()] ptr mem))

// medium and large zeroing uses a loop; no duff's device implemented yet
(Zero [s] ptr mem)
	&& SizeAndAlign(s).Size()%8 == 0 && (SizeAndAlign(s).Size() > 32 || config.noDuffDevice) ->
	(LoweredZero
		ptr
		(ADDconst <ptr.Type> [SizeAndAlign(s).Size()-moveSize(SizeAndAlign(s).Align(), config)] ptr)
		mem)

// store zero
(MOVBstore [off] {sym} ptr (MOVWconst [0]) mem) -> (MOVBstorezero [off] {sym} ptr mem)
(MOVBstore [off] {sym} ptr (MOVDconst [0]) mem) -> (MOVBstorezero [off] {sym} ptr mem)
(MOVHstore [off] {sym} ptr (MOVWconst [0]) mem) -> (MOVHstorezero [off] {sym} ptr mem)
(MOVHstore [off] {sym} ptr (MOVDconst [0]) mem) -> (MOVHstorezero [off] {sym} ptr mem)
(MOVWstore [off] {sym} ptr (MOVWconst [0]) mem) -> (MOVWstorezero [off] {sym} ptr mem)
(MOVWstore [off] {sym} ptr (MOVDconst [0]) mem) -> (MOVWstorezero [off] {sym} ptr mem)
(MOVDstore [off] {sym} ptr (MOVWconst [0]) mem) -> (MOVDstorezero [off] {sym} ptr mem)
(MOVDstore [off] {sym} ptr (MOVDconst [0]) mem) -> (MOVDstorezero [off] {sym} ptr mem)

// moves
(Move [s] _ _ mem) && SizeAndAlign(s).Size() == 0 -> mem
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 1 -> (MOVBstore dst (MOVUBload src mem) mem)
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 2 -> (MOVHstore dst (MOVUHload src mem) mem)
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 4 -> (MOVWstore dst (MOVUWload src mem) mem)
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 8 -> (MOVDstore dst (MOVDload src mem) mem)

(Move [s] dst src mem) && SizeAndAlign(s).Size() == 3 ->
	(MOVBstore [2] dst (MOVUBload [2] src mem)
		(MOVHstore dst (MOVUHload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 5 ->
	(MOVBstore [4] dst (MOVUBload [4] src mem)
		(MOVWstore dst (MOVUWload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 6 ->
	(MOVHstore [4] dst (MOVUHload [4] src mem)
		(MOVWstore dst (MOVUWload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 7 ->
	(MOVBstore [6] dst (MOVUBload [6] src mem)
		(MOVHstore [4] dst (MOVUHload [4] src mem)
			(MOVWstore dst (MOVUWload src mem) mem)))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 12 ->
	(MOVWstore [8] dst (MOVUWload [8] src mem)
		(MOVDstore dst (MOVDload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 16 ->
	(MOVDstore [8] dst (MOVDload [8] src mem)
		(MOVDstore dst (MOVDload src mem) mem))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 24 ->
	(MOVDstore [16] dst (MOVDload [16] src mem)
		(MOVDstore [8] dst (MOVDload [8] src mem)
			(MOVDstore dst (MOVDload src mem) mem)))
(Move [s] dst src mem) && SizeAndAlign(s).Size() == 32 ->
	(MOVDstore [24] dst (MOVDload [24] src mem)
		(MOVDstore [16] dst (MOVDload [16] src mem)
			(MOVDstore [8] dst (MOVDload [8] src mem)
				(MOVDstore dst (MOVDload src mem) mem))))

// strip off fractional word move
(Move [s] dst src mem) && SizeAndAlign(s).Size()%8 != 0 && SizeAndAlign(s).Size() > 8 ->
	(Move [MakeSizeAndAlign(SizeAndAlign(s).Size()%8, 1).Int64()]
		(OffPtr <dst.Type> dst [SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8])
		(OffPtr <src.Type> src [SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8])
		(Move [MakeSizeAndAlign(SizeAndAlign(s).Size()-SizeAndAlign(s).Size()%8, 1).Int64()] dst src mem))

// large move uses a loop
// DUFFCOPY is not implemented on SPARC64 (TODO)
(Move [s] dst src mem)
	&& SizeAndAlign(s).Size() > 32 && SizeAndAlign(s).Size()%8 == 0 ->
	(LoweredMove
		dst
		src
		(ADDconst <src.Type> src [SizeAndAlign(s).Size()-moveSize(SizeAndAlign(s).Align(), config)])
		mem)

// comparisons
(Eq8 x y)  -> (Equal32 (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Eq16 x y) -> (Equal32 (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Eq32 x y) -> (Equal32 (CMP x y))
(Eq64 x y) -> (Equal64 (CMP x y))
(EqPtr x y) -> (Equal64 (CMP x y))
(Eq32F x y) -> (EqualF (FCMPS x y))
(Eq64F x y) -> (EqualF (FCMPD x y))

(Neq8 x y)  -> (NotEqual32 (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Neq16 x y) -> (NotEqual32 (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Neq32 x y) -> (NotEqual32 (CMP x y))
(Neq64 x y) -> (NotEqual64 (CMP x y))
(NeqPtr x y) -> (NotEqual64 (CMP x y))
(Neq32F x y) -> (NotEqualF (FCMPS x y))
(Neq64F x y) -> (NotEqualF (FCMPD x y))

(Less8 x y)  -> (LessThan32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Less16 x y) -> (LessThan32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Less32 x y) -> (LessThan32 (CMP x y))
(Less64 x y) -> (LessThan64 (CMP x y))
(Less8U x y)  -> (LessThan32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Less16U x y) -> (LessThan32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Less32U x y) -> (LessThan32U (CMP x y))
(Less64U x y) -> (LessThan64U (CMP x y))
(Less32F x y) -> (LessThanF (FCMPS x y))
(Less64F x y) -> (LessThanF (FCMPD x y))

(Leq8 x y)  -> (LessEqual32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Leq16 x y) -> (LessEqual32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Leq32 x y) -> (LessEqual32 (CMP x y))
(Leq64 x y) -> (LessEqual64 (CMP x y))
(Leq8U x y)  -> (LessEqual32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Leq16U x y) -> (LessEqual32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Leq32U x y) -> (LessEqual32U (CMP x y))
(Leq64U x y) -> (LessEqual64U (CMP x y))
(Leq32F x y) -> (LessEqualF (FCMPS x y))
(Leq64F x y) -> (LessEqualF (FCMPD x y))

(Greater8 x y)  -> (GreaterThan32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Greater16 x y) -> (GreaterThan32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Greater32 x y) -> (GreaterThan32 (CMP x y))
(Greater64 x y) -> (GreaterThan64 (CMP x y))
(Greater8U x y)  -> (GreaterThan32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Greater16U x y) -> (GreaterThan32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Greater32U x y) -> (GreaterThan32U (CMP x y))
(Greater64U x y) -> (GreaterThan64U (CMP x y))
(Greater32F x y) -> (GreaterThanF (FCMPS x y))
(Greater64F x y) -> (GreaterThanF (FCMPD x y))

(Geq8 x y)  -> (GreaterEqual32 (CMP (SignExt8to32 x) (SignExt8to32 y)))
(Geq16 x y) -> (GreaterEqual32 (CMP (SignExt16to32 x) (SignExt16to32 y)))
(Geq32 x y) -> (GreaterEqual32 (CMP x y))
(Geq64 x y) -> (GreaterEqual64 (CMP x y))
(Geq8U x y)  -> (GreaterEqual32U (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
(Geq16U x y) -> (GreaterEqual32U (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
(Geq32U x y) -> (GreaterEqual32U (CMP x y))
(Geq64U x y) -> (GreaterEqual64U (CMP x y))
(Geq32F x y) -> (GreaterEqualF (FCMPS x y))
(Geq64F x y) -> (GreaterEqualF (FCMPD x y))

// checks
(NilCheck ptr mem) -> (LoweredNilCheck ptr mem)
(IsNonNil ptr) -> (NotEqual64 (CMPconst [0] ptr))
(IsInBounds idx len) -> (LessThan64U (CMP idx len))
(IsSliceInBounds idx len) -> (LessEqual64U (CMP idx len))

// pseudo-ops
(GetClosurePtr) -> (LoweredGetClosurePtr)
(Convert x mem) -> (MOVDconvert x mem)

// Absorb pseudo-ops into blocks.
(If (Equal32 cc) yes no) -> (EW cc yes no)
(If (NotEqual32 cc) yes no) -> (NEW cc yes no)
(If (LessThan32 cc) yes no) -> (LW cc yes no)
(If (LessThan32U cc) yes no) -> (CSW cc yes no)
(If (LessEqual32 cc) yes no) -> (LEW cc yes no)
(If (LessEqual32U cc) yes no) -> (LEUW cc yes no)
(If (GreaterThan32 cc) yes no) -> (GW cc yes no)
(If (GreaterThan32U cc) yes no) -> (GUW cc yes no)
(If (GreaterEqual32 cc) yes no) -> (GEW cc yes no)
(If (GreaterEqual32U cc) yes no) -> (CCW cc yes no)

(If (Equal64 cc) yes no) -> (ED cc yes no)
(If (NotEqual64 cc) yes no) -> (NED cc yes no)
(If (LessThan64 cc) yes no) -> (LD cc yes no)
(If (LessThan64U cc) yes no) -> (CSD cc yes no)
(If (LessEqual64 cc) yes no) -> (LED cc yes no)
(If (LessEqual64U cc) yes no) -> (LEUD cc yes no)
(If (GreaterThan64 cc) yes no) -> (GD cc yes no)
(If (GreaterThan64U cc) yes no) -> (GUD cc yes no)
(If (GreaterEqual64 cc) yes no) -> (GED cc yes no)
(If (GreaterEqual64U cc) yes no) -> (CCD cc yes no)

(If (EqualF cc) yes no) -> (EF cc yes no)
(If (NotEqualF cc) yes no) -> (NEF cc yes no)
(If (LessThanF cc) yes no) -> (LF cc yes no)
(If (LessEqualF cc) yes no) -> (LEF cc yes no)
(If (GreaterThanF cc) yes no) -> (GF cc yes no)
(If (GreaterEqualF cc) yes no) -> (GEF cc yes no)

(If cond yes no) -> (NEW (CMPconst [0] cond) yes no)

// Absorb boolean tests into block
(NEW (CMPconst [0] (Equal32 cc)) yes no) -> (EW cc yes no)
(NEW (CMPconst [0] (NotEqual32 cc)) yes no) -> (NEW cc yes no)
(NEW (CMPconst [0] (LessThan32 cc)) yes no) -> (LW cc yes no)
(NEW (CMPconst [0] (LessThan32U cc)) yes no) -> (CSW cc yes no)
(NEW (CMPconst [0] (LessEqual32 cc)) yes no) -> (LEW cc yes no)
(NEW (CMPconst [0] (LessEqual32U cc)) yes no) -> (LEUW cc yes no)
(NEW (CMPconst [0] (GreaterThan32 cc)) yes no) -> (GW cc yes no)
(NEW (CMPconst [0] (GreaterThan32U cc)) yes no) -> (GUW cc yes no)
(NEW (CMPconst [0] (GreaterEqual32 cc)) yes no) -> (GEW cc yes no)
(NEW (CMPconst [0] (GreaterEqual32U cc)) yes no) -> (CCW cc yes no)

(NEW (CMPconst [0] (Equal64 cc)) yes no) -> (ED cc yes no)
(NEW (CMPconst [0] (NotEqual64 cc)) yes no) -> (NED cc yes no)
(NEW (CMPconst [0] (LessThan64 cc)) yes no) -> (LD cc yes no)
(NEW (CMPconst [0] (LessThan64U cc)) yes no) -> (CSD cc yes no)
(NEW (CMPconst [0] (LessEqual64 cc)) yes no) -> (LED cc yes no)
(NEW (CMPconst [0] (LessEqual64U cc)) yes no) -> (LEUD cc yes no)
(NEW (CMPconst [0] (GreaterThan64 cc)) yes no) -> (GD cc yes no)
(NEW (CMPconst [0] (GreaterThan64U cc)) yes no) -> (GUD cc yes no)
(NEW (CMPconst [0] (GreaterEqual64 cc)) yes no) -> (GED cc yes no)
(NEW (CMPconst [0] (GreaterEqual64U cc)) yes no) -> (CCD cc yes no)

(NEW (CMPconst [0] (EqualF cc)) yes no) -> (EF cc yes no)
(NEW (CMPconst [0] (NotEqualF cc)) yes no) -> (NEF cc yes no)
(NEW (CMPconst [0] (LessThanF cc)) yes no) -> (LF cc yes no)
(NEW (CMPconst [0] (LessEqualF cc)) yes no) -> (LEF cc yes no)
(NEW (CMPconst [0] (GreaterThanF cc)) yes no) -> (GF cc yes no)
(NEW (CMPconst [0] (GreaterEqualF cc)) yes no) -> (GEF cc yes no)

// fold offset into address
(ADDconst [off1] (MOVDaddr [off2] {sym} ptr)) -> (MOVDaddr [off1+off2] {sym} ptr)

// fold offsets for storezero
(MOVBstorezero [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVBstorezero [off1+off2] {sym} ptr mem)
(MOVHstorezero [off1] {sym} (ADDconst [off2] ptr) mem)
	&& (off1+off2)%2==0 || off1+off2<4096 && off1+off2>-4097 && !isArg(sym) && !isAuto(sym) ->
	(MOVHstorezero [off1+off2] {sym} ptr mem)
(MOVWstorezero [off1] {sym} (ADDconst [off2] ptr) mem)
	&& (off1+off2)%4==0 || off1+off2<4096 && off1+off2>-4097 && !isArg(sym) && !isAuto(sym) ->
	(MOVWstorezero [off1+off2] {sym} ptr mem)
(MOVDstorezero [off1] {sym} (ADDconst [off2] ptr) mem)
	&& (off1+off2)%8==0 || off1+off2<4096 && off1+off2>-4097 && !isArg(sym) && !isAuto(sym) ->
	(MOVDstorezero [off1+off2] {sym} ptr mem)

// generic simplifications
(ADD x (NEG y)) -> (SUB x y)
(ADD (NEG y) x) -> (SUB x y)
(SUB x x) -> (MOVDconst [0])
(AND x x) -> x
(OR  x x) -> x
(XOR x x) -> (MOVDconst [0])

// remove redundant *const ops
(ADDconst [0]  x) -> x
(SUBconst [0]  x) -> x
(ANDconst [0]  _) -> (MOVDconst [0])
(ANDconst [-1] x) -> x
(ORconst  [0]  x) -> x
(ORconst  [-1] _) -> (MOVDconst [-1])
(XORconst [0]  x) -> x

// fold constant into arithmatic ops
(ADD (MOVDconst [c]) x) -> (ADDconst [c] x)
(ADD x (MOVDconst [c])) -> (ADDconst [c] x)
(SUB x (MOVDconst [c])) -> (SUBconst [c] x)
(AND (MOVDconst [c]) x) -> (ANDconst [c] x)
(AND x (MOVDconst [c])) -> (ANDconst [c] x)
(OR  (MOVDconst [c]) x) -> (ORconst  [c] x)
(OR  x (MOVDconst [c])) -> (ORconst  [c] x)
(XOR (MOVDconst [c]) x) -> (XORconst [c] x)
(XOR x (MOVDconst [c])) -> (XORconst [c] x)

(ADD (MOVWconst [c]) x) -> (ADDconst [c] x)
(ADD x (MOVWconst [c])) -> (ADDconst [c] x)
(SUB x (MOVWconst [c])) -> (SUBconst [c] x)
(AND (MOVWconst [c]) x) -> (ANDconst [c] x)
(AND x (MOVWconst [c])) -> (ANDconst [c] x)
(OR  (MOVWconst [c]) x) -> (ORconst  [c] x)
(OR  x (MOVWconst [c])) -> (ORconst  [c] x)
(XOR (MOVWconst [c]) x) -> (XORconst [c] x)
(XOR x (MOVWconst [c])) -> (XORconst [c] x)

(SLL x (MOVDconst [c])) -> (SLLconst x [c&63]) // Note: I don't think we ever generate bad constant shifts (i.e. c>=64)
(SRL x (MOVDconst [c])) -> (SRLconst x [c&63])
(SRA x (MOVDconst [c])) -> (SRAconst x [c&63])

(SLL x (MOVWconst [c])) -> (SLLconst x [c&63]) // Note: I don't think we ever generate bad constant shifts (i.e. c>=64)
(SRL x (MOVWconst [c])) -> (SRLconst x [c&63])
(SRA x (MOVWconst [c])) -> (SRAconst x [c&63])

(CMP x (MOVDconst [c])) -> (CMPconst [c] x)
(CMP x (MOVWconst [c])) -> (CMPconst [c] x)

// generic constant folding
(ADDconst [c] (MOVDconst [d]))  -> (MOVDconst [c+d])
(ADDconst [c] (MOVWconst [d]))  -> (MOVWconst [c+d])
(ADDconst [c] (ADDconst [d] x)) -> (ADDconst [c+d] x)
(ADDconst [c] (SUBconst [d] x)) -> (ADDconst [c-d] x)

(SUBconst [c] (MOVDconst [d]))  -> (MOVDconst [d-c])
(SUBconst [c] (MOVWconst [d]))  -> (MOVWconst [d-c])
(SUBconst [c] (SUBconst [d] x)) -> (ADDconst [-c-d] x)
(SUBconst [c] (ADDconst [d] x)) -> (ADDconst [-c+d] x)
